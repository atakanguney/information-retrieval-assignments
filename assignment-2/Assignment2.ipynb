{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** Do not change signatures of methods defined below. Those methods will be used while grading your homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from heapq import nlargest\n",
    "from itertools import islice\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_ID_FILE = \"movie_ids.csv\"\n",
    "ALL_MOVIE_CONTENTS = \"all_movie_contents.pickle\"\n",
    "BASE_URL = \"https://www.imdb.com/title/\"\n",
    "NUMBER_WORDS = 5000\n",
    "NUMBER_RECS = 10\n",
    "STOPWORDS = []# ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    \"\"\"\n",
    "    Read csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        File to read\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(imdb_id):\n",
    "    \"\"\"Get HTML content of movie\n",
    "    \"\"\"\n",
    "    url = BASE_URL + imdb_id\n",
    "    response = requests.get(url)\n",
    "    if response.status_code >= 400:\n",
    "        raise ValueError(\"Something went wrong in request !: {}, {}\".format(url, response.status_code))\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title(html_content):\n",
    "    # TODO: Write doc-string\n",
    "    pattern = re.compile(r'<div\\s+?class\\s*?=\\s*?\"title_wrapper\".*?'\n",
    "                         + r'<h1\\s*?class\\s*?=\\s*?\".*?\">(.*?)&nbsp;', re.DOTALL)\n",
    "    match = pattern.search(html_content)\n",
    "    \n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rec_item(html_content):\n",
    "    # TODO: Write doc-string\n",
    "    pattern = re.compile(r'<div\\s+?class\\s*?=\\s*?\"(?:rec_item|rec_item rec_selected)\" data-info=\"\" data-spec=\".*?\" data-tconst=\"(.*?)\">')\n",
    "    for match in pattern.finditer(html_content):\n",
    "        yield match.group(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recommendations(html_content):\n",
    "    # TODO: Write doc-string\n",
    "    return list(find_rec_item(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_storyline(html_content):\n",
    "    # TODO: Write doc-string\n",
    "    pattern = re.compile(r'<h2>\\s*?Storyline\\s*?</h2>.*?'\n",
    "                         + r'<span>(.*?)</span>', re.DOTALL)\n",
    "\n",
    "    match = pattern.search(html_content)\n",
    "\n",
    "    return match.group(1).strip() # + \" \" + match.group(2).strip() + \" \" + match.group(3).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_contents(imdb_id):\n",
    "    \"\"\"\n",
    "    Gets an imdb id and returns its title, storyline, list of IMDB recommendations respectively.\n",
    "    \"\"\"\n",
    "    html_content = get_html_content(imdb_id)\n",
    "    try:\n",
    "        title = find_title(html_content)\n",
    "        storyline = find_storyline(html_content)\n",
    "        recommendations = find_recommendations(html_content)\n",
    "    except AttributeError as e:\n",
    "        print(html_content)\n",
    "        raise AttributeError\n",
    "\n",
    "    return title, storyline, recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(path, pickle_path, restore=False):\n",
    "    # TODO: Write doc-string\n",
    "    movie_ids = read_csv(path)\n",
    "    if restore:\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            all_movie_contents = pickle.load(f)\n",
    "    else:\n",
    "        all_movie_contents = {}\n",
    "    for i, movie_id in enumerate(movie_ids):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Movie id: {}\".format(movie_id))\n",
    "            print(\"Number of collected movie: {}\".format(len(all_movie_contents)))\n",
    "        if movie_id in all_movie_contents:\n",
    "            continue\n",
    "        while True:\n",
    "            try:\n",
    "                title, storyline, recs = get_movie_contents(movie_id)\n",
    "            except ConnectionError as e:\n",
    "                time.sleep(4)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                with open(pickle_path, \"wb\") as f:\n",
    "                    pickle.dump(all_movie_contents, f)\n",
    "                raise e\n",
    "            break\n",
    "\n",
    "        movie_content = MovieContent(\n",
    "            title=title,\n",
    "            storyline=storyline,\n",
    "            recommendations=recs\n",
    "        )\n",
    "        all_movie_contents[movie_id] = movie_content\n",
    "        for rec_id in recs:\n",
    "            if rec_id in all_movie_contents:\n",
    "                continue\n",
    "            while True:\n",
    "                try:\n",
    "                    rec_title, rec_storyline, rec_recs = get_movie_contents(rec_id)\n",
    "                except ConnectionError as e:\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    with open(pickle_path, \"wb\") as f:\n",
    "                        pickle.dump(all_movie_contents, f)\n",
    "                    raise e\n",
    "                break\n",
    "\n",
    "            rec_movie_content = MovieContent(\n",
    "                title=rec_title,\n",
    "                storyline=rec_storyline,\n",
    "                recommendations=rec_recs\n",
    "            )\n",
    "            all_movie_contents[rec_id] = rec_movie_content\n",
    "\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(all_movie_contents, f)\n",
    "\n",
    "    return all_movie_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovieContent = namedtuple(\"MovieContent\", \"title storyline recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movie_contents = scraping(MOVIE_ID_FILE, ALL_MOVIE_CONTENTS, restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ALL_MOVIE_CONTENTS, \"rb\") as f:\n",
    "    all_movie_contents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htlm_tags(doc):\n",
    "    \"\"\"Clean links\"\"\"\n",
    "    pattern = re.compile(r\"<[/]?a.*?>\", re.DOTALL)\n",
    "    return re.sub(pattern, \"\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, stopwords):\n",
    "    \"\"\"Tokenize document\"\"\"\n",
    "    doc = doc.lower()\n",
    "    doc = remove_htlm_tags(doc)\n",
    "    pattern = re.compile(r\"\\w+\")\n",
    "    \n",
    "    return [match.group() for match in pattern.finditer(doc) if match and match.group() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_corpus(contents, stopwords):\n",
    "    \"\"\"Construct corpus with given movie contents\"\"\"\n",
    "    return {\n",
    "        movie_id: tokenize(content.storyline, stopwords)\n",
    "        for movie_id, content in contents.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vocabulary(corpus):\n",
    "    \"\"\"Construct vocabulary by given corpus\"\"\"\n",
    "    vocab = {}\n",
    "    for doc_id, doc in corpus.items():\n",
    "        for word, count in dict(Counter(doc)).items():\n",
    "            vocab.setdefault(word, []).append((doc_id, count))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    return dict(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(vocabulary):\n",
    "    return {\n",
    "        word: reduce(lambda total_count, doc_count: total_count + doc_count[1], doc_count_list, 0)\n",
    "        for word, doc_count_list in vocabulary.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_words(k, vocabulary):\n",
    "    \"\"\"Returns most frequent k words\"\"\"\n",
    "    vocab_occurences = get_occurences(vocabulary)\n",
    "    return sorted(vocab_occurences, key=lambda x: vocab_occurences[x], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(sparse_vector):\n",
    "    \"\"\"Calculate L2 norm of given sparse vector\"\"\"\n",
    "    return (reduce(lambda sum_, score: sum_ + score**2, sparse_vector.values(), 0))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm({\"w1\": 6, \"w2\": 8, \"w3\": 125**0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(vocabulary, top_k_words, num_docs):\n",
    "    \"\"\"Compute document vectors\"\"\"\n",
    "    doc2vec = {}\n",
    "    for word in top_k_words:\n",
    "        doc_count_list = vocabulary[word]\n",
    "        document_freq = len(doc_count_list)\n",
    "        for doc, term_freq in doc_count_list:\n",
    "            doc2vec.setdefault(doc, {}).update({word: term_freq * (log(num_docs) - log(document_freq))})\n",
    "\n",
    "\n",
    "    for doc_id, raw_tf_idf_vector in doc2vec.items():\n",
    "        vector_norm = norm(raw_tf_idf_vector)\n",
    "        doc2vec[doc_id] = {\n",
    "            word: raw_tf_idf / vector_norm\n",
    "            for word, raw_tf_idf in raw_tf_idf_vector.items()\n",
    "        }\n",
    "        \n",
    "    return doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = construct_corpus(all_movie_contents, STOPWORDS)\n",
    "\n",
    "vocabulary = construct_vocabulary(corpus)\n",
    "\n",
    "vocab_occurences = get_occurences(vocabulary)\n",
    "\n",
    "top_n_words = top_k_words(NUMBER_WORDS, vocabulary)\n",
    "\n",
    "num_docs = len(corpus)\n",
    "\n",
    "doc_vec = doc2vec(vocabulary, top_n_words, num_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vector(text):\n",
    "    tokens = tokenize(text, STOPWORDS)\n",
    "    raw_vector = {\n",
    "        word: tokens.count(word)\n",
    "        for word in set(tokens)\n",
    "    }\n",
    "    vector_norm = norm(raw_vector)\n",
    "    return {\n",
    "        word: score / vector_norm\n",
    "        for word, score in raw_vector.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dot_product(sparse_vec_1, sparse_vec_2):\n",
    "    common_words = set(sparse_vec_1).intersection(set(sparse_vec_2))\n",
    "    result = 0\n",
    "    for word in common_words:\n",
    "        result += sparse_vec_1[word] * sparse_vec_2[word]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_k_similar(k, text_vector, doc_vec):\n",
    "    similarities = {\n",
    "        doc_id: sparse_dot_product(doc_vector, text_vector)\n",
    "        for doc_id, doc_vector in doc_vec.items()\n",
    "    }\n",
    "    return list(nlargest(k, similarities, key=similarities.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = NUMBER_RECS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(imdb_id):\n",
    "    \"\"\"\n",
    "    Gets an imdb id and returns a list of recommended movie ids for that movie. \n",
    "    \"\"\"\n",
    "    storyline = all_movie_contents[imdb_id].storyline\n",
    "    \n",
    "    text_vector = construct_vector(storyline)\n",
    "    \n",
    "    return get_most_k_similar(K, text_vector, doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(rec_movie_ids, relevant_movie_ids):\n",
    "    \"\"\"Compute Precision\"\"\"\n",
    "    count = 0\n",
    "    for movie_id in relevant_movie_ids:\n",
    "        if movie_id in rec_movie_ids:\n",
    "            count += 1\n",
    "\n",
    "    return count / (len(rec_movie_ids) + 10**-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(rec_movie_ids, relevant_movie_ids):\n",
    "    \"\"\"Compute Recall\"\"\"\n",
    "    count = 0\n",
    "    for movie_id in rec_movie_ids:\n",
    "        if movie_id in relevant_movie_ids:\n",
    "            count += 1\n",
    "            \n",
    "    return count / (len(relevant_movie_ids) + 10**-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(rec_movie_ids, relevant_movie_ids, K):\n",
    "    \"\"\"\n",
    "    Gets list of recommended and relevant movie ids and K value.\n",
    "    \n",
    "    Returns precision, recall, F1 values for K respectively. \n",
    "    \"\"\"\n",
    "    prec = precision(rec_movie_ids, relevant_movie_ids) \n",
    "    rec = recall(rec_movie_ids, relevant_movie_ids)\n",
    "    F_1 = 2 * prec * rec / (prec + rec + 10**(-8))\n",
    "    \n",
    "    return prec, rec, F_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(rec_movie_ids, relevant_movie_ids):\n",
    "    \"\"\"Calculates average precision\"\"\"\n",
    "    sum_prec = 0\n",
    "    relevant_count = 0\n",
    "    for i, rec in enumerate(rec_movie_ids):\n",
    "        if rec in relevant_movie_ids:\n",
    "            relevant_count += 1\n",
    "            sum_prec += relevant_count / (i + 1)\n",
    "    return sum_prec / (len(relevant_movie_ids) + 10**-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = read_csv(MOVIE_ID_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs = []\n",
    "recalls = []\n",
    "f_1s = []\n",
    "for movie_id in movie_ids:\n",
    "    rec_movie_ids = recommend(movie_id)\n",
    "    relevant_movie_ids = all_movie_contents[movie_id].recommendations\n",
    "    prec, rec, f_1 = evaluate_recommendations(rec_movie_ids, relevant_movie_ids, NUMBER_RECS)\n",
    "    precs.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f_1s.append(f_1)\n",
    "    if prec > 0:\n",
    "        print(\"Precision: {}, Recall: {}, F-1: {}\".format(prec, rec, f_1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ap = 0\n",
    "for movie_id in movie_ids:\n",
    "    rec_movie_ids = recommend(movie_id)\n",
    "    if not rec_movie_ids:\n",
    "        print(movie_id)\n",
    "    relevant_movie_ids = all_movie_contents[movie_id].recommendations\n",
    "    sum_ap += average_precision(rec_movie_ids, relevant_movie_ids)\n",
    "print(\"Mean Average Precision :{}\".format(sum_ap / len(movie_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
