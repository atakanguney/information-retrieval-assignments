{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** Do not change signatures of methods defined below. Those methods will be used while grading your homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from heapq import nlargest\n",
    "from itertools import islice\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_ID_FILE = \"movie_ids.csv\"\n",
    "ALL_MOVIE_CONTENTS = \"all_movie_contents.pickle\"\n",
    "BASE_URL = \"https://www.imdb.com/title/\"\n",
    "NUMBER_WORDS = 5000\n",
    "NUMBER_RECS = 12\n",
    "TF_IDF_THRESHOLD = 10.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading movie ids from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    \"\"\"Read csv file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        File to read\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of IMDB ids read from file\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines if line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Request to IMDB\n",
    "\n",
    "-  It requires `requests` library to make `GET` reqest to IMDB\n",
    "-  Link format is https://www.imdb.com/title/{imdb_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(imdb_id):\n",
    "    \"\"\"Get HTML content of movie\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imdb_id: str\n",
    "        Movie id\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Raw text, which should be in html format\n",
    "    \"\"\"\n",
    "    url = BASE_URL + imdb_id\n",
    "    response = requests.get(url)\n",
    "    if response.status_code >= 400:\n",
    "        raise ValueError(\"Something went wrong in request !: {}, {}\".format(url, response.status_code))\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_title(html_content):\n",
    "    \"\"\"Find title in given html content\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html_content: str\n",
    "        Raw html text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str:\n",
    "        Matched title in html text\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'<div\\s+?class\\s*?=\\s*?\"title_wrapper\".*?'\n",
    "                         + r'<h1\\s*?class\\s*?=\\s*?\".*?\">(.*?)&nbsp;', re.DOTALL)\n",
    "    match = pattern.search(html_content)\n",
    "    \n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recommendations(html_content):\n",
    "    \"\"\"Find recommended items\n",
    "\n",
    "    It uses regex to extract recommendation item in `html_content`.\n",
    "    In regex pattern, I have concerned that all recommendation items are\n",
    "    in a <div> tag which has some attributes specific to recommendation items\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html_content: str\n",
    "        Raw html text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Matched all recommended items in html text\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'<div\\s+?class\\s*?=\\s*?\"(?:rec_item|rec_item rec_selected)\" data-info=\"\" data-spec=\".*?\" data-tconst=\"(.*?)\">')\n",
    "    \n",
    "    return pattern.findall(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_storyline(html_content):\n",
    "    \"\"\"Find storylin in given html content\n",
    "    \n",
    "    By using regex expression, this function extracts storyline of movie\n",
    "    from given `html_content`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    html_content: str\n",
    "        Raw html text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Matched storyline in `html_content`\n",
    "    \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'<h2>\\s*?Storyline\\s*?</h2>.*?'\n",
    "                         + r'<span>(.*?)</span>', re.DOTALL)\n",
    "\n",
    "    match = pattern.search(html_content)\n",
    "\n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_contents(imdb_id):\n",
    "    \"\"\"\n",
    "    Gets an imdb id and returns its title, storyline, list of IMDB recommendations respectively.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imdb_id: str\n",
    "        IMDB id of the movie\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    title: str\n",
    "        Title of the movie\n",
    "    storyline: str\n",
    "        Storyline of the movie\n",
    "    recommendations: list\n",
    "        List of recommended films on IMDB website\n",
    "    \"\"\"\n",
    "    html_content = get_html_content(imdb_id)\n",
    "    try:\n",
    "        title = find_title(html_content)\n",
    "        storyline = find_storyline(html_content)\n",
    "        recommendations = find_recommendations(html_content)\n",
    "    except AttributeError as e:\n",
    "        print(html_content)\n",
    "        raise AttributeError\n",
    "\n",
    "    return title, storyline, recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Procedure\n",
    "\n",
    "In the scraping procedure I have followed the procedure:\n",
    "\n",
    "1. Get all movie ids from a file\n",
    "2. Create or Restore a dictionary that keeps movie contents, then for each movie id that is not in this dictionary:\n",
    "2. Get html content from imdb\n",
    "3. Get contents from html content, which are title, storyline, recommendations\n",
    "4. Store it to the dictionary\n",
    "\n",
    "### Notes\n",
    "\n",
    "-  IMDB website has a limit on number of requests, that is why in scraping method,if a `ConnectionError` happens, the program will try to get content again, after waiting 10 seconds\n",
    "-  To be able to scrape partially, if something bad happens when try to getting contents of a movie, scraping method save immediate dictionary, so you could restore it from where it stops\n",
    "-  As prerequisite, you <b>MUST</b> declare `MovieContent` namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(path, pickle_path, restore=False):\n",
    "    \"\"\"Whole scraping procedure\n",
    "    \n",
    "    It first gets all movie ids from a csv file in `path`,\n",
    "    then if `restore` is True, it restores pickle file from `pickle_path`, else\n",
    "    it starts to keep scraped data in new generated dictionary. This dictionary,\n",
    "    all movie contents that are scraped are kept in following format:\n",
    "    \n",
    "    {\n",
    "        {imdb_id}: MovieContent(\n",
    "            title={movie_title},\n",
    "            storyline={movie_storyline},\n",
    "            recommendations={movie_recommendations}\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    Since I have used a namedtuple `MovieContent`, before calling this function,\n",
    "    A namedtuple MovieContent MUST be declared, i.e. see following cells\n",
    "    \n",
    "    After declaring `all_movie_contents` dict this method gets all contents for movie ids\n",
    "    got from csv file and for the movie ids in their recommendation list\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to read movie ids\n",
    "    pickle_path: str\n",
    "        Path to pickle file, which is used both restore and save contents\n",
    "    restore: bool\n",
    "        Decided restore from `pickle_path` or start from scratch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_movie_contens: dict\n",
    "        All contents of movies\n",
    "    \"\"\"\n",
    "    movie_ids = read_csv(path)\n",
    "    if restore:\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            all_movie_contents = pickle.load(f)\n",
    "    else:\n",
    "        all_movie_contents = {}\n",
    "    for i, movie_id in enumerate(movie_ids):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Movie id: {}\".format(movie_id))\n",
    "            print(\"Number of collected movie: {}\".format(len(all_movie_contents)))\n",
    "        if movie_id in all_movie_contents:\n",
    "            continue\n",
    "        while True:\n",
    "            try:\n",
    "                title, storyline, recs = get_movie_contents(movie_id)\n",
    "            except ConnectionError as e:\n",
    "                time.sleep(4)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                with open(pickle_path, \"wb\") as f:\n",
    "                    pickle.dump(all_movie_contents, f)\n",
    "                raise e\n",
    "            break\n",
    "\n",
    "        movie_content = MovieContent(\n",
    "            title=title,\n",
    "            storyline=storyline,\n",
    "            recommendations=recs\n",
    "        )\n",
    "        all_movie_contents[movie_id] = movie_content\n",
    "        for rec_id in recs:\n",
    "            if rec_id in all_movie_contents:\n",
    "                continue\n",
    "            while True:\n",
    "                try:\n",
    "                    rec_title, rec_storyline, rec_recs = get_movie_contents(rec_id)\n",
    "                except ConnectionError as e:\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    with open(pickle_path, \"wb\") as f:\n",
    "                        pickle.dump(all_movie_contents, f)\n",
    "                    raise e\n",
    "                break\n",
    "\n",
    "            rec_movie_content = MovieContent(\n",
    "                title=rec_title,\n",
    "                storyline=rec_storyline,\n",
    "                recommendations=rec_recs\n",
    "            )\n",
    "            all_movie_contents[rec_id] = rec_movie_content\n",
    "\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(all_movie_contents, f)\n",
    "\n",
    "    return all_movie_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovieContent = namedtuple(\"MovieContent\", \"title storyline recommendations\")\n",
    "\n",
    "# To scrape whole data uncomment below line\n",
    "# all_movie_contents = scraping(MOVIE_ID_FILE, ALL_MOVIE_CONTENTS, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have already scraped data, you can load from pickle file\n",
    "with open(ALL_MOVIE_CONTENTS, \"rb\") as f:\n",
    "    all_movie_contents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model\n",
    "\n",
    "Tf-idf model requires some preprocessing on content data\n",
    "\n",
    "-  Constructing corpus\n",
    "-  Constructing vocabulary, with document frequencies\n",
    "-  Selecting words to represent documents\n",
    "    -  To select words, I have set a threshold for tf-idf score, which is 10.7\n",
    "    -  I have selected this threshold because if a word occurs once in only one document, their tf-idf scores will be $1 \\times \\log{\\frac{N}{1}} \\approx 10.61$ where $N = 2820$ for my corpus\n",
    "-  Creating document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htlm_tags(doc):\n",
    "    \"\"\"Clean links\n",
    "    \n",
    "    Some storylines contain <a> html tag, they should be removed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: str\n",
    "        Document to remove <a> html tags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        HTML tag <a> removed version of doc\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"<[/]?a.*?>\", re.DOTALL)\n",
    "    return re.sub(pattern, \"\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, stopwords=[]):\n",
    "    \"\"\"Tokenize document\n",
    "    \n",
    "    Tokenize document and use `stopwords` to exclude some tokens.\n",
    "    This tokenization process ignores puntuations and it considers\n",
    "    only portions that contains alphanumeric characters and underscore\n",
    "    \n",
    "    Note: Before tokenization process, it lowers all the characters in `doc`\n",
    "    and removes <a> tag in `doc`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: str\n",
    "        Document to tokenize\n",
    "    stopwords: list\n",
    "        Lowercased stopword list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Tokenized version of document as token list\n",
    "    \"\"\"\n",
    "    doc = doc.lower()\n",
    "    doc = remove_htlm_tags(doc)\n",
    "    pattern = re.compile(r\"\\w+\")\n",
    "    \n",
    "    return [match.group() for match in pattern.finditer(doc) if match and match.group() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_corpus(contents, stopwords=[]):\n",
    "    \"\"\"Construct corpus with given movie contents\n",
    "    \n",
    "    For each movie in `contents` dictionary program tokenize its\n",
    "    storyline, and store it in a dictionary\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    contents: dict\n",
    "        Dictionary of movie_id: movie_contents\n",
    "    stopwords: list\n",
    "        List of stopwords, default is empty\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Movie id as key, tokenized storyline as value\n",
    "    \"\"\"\n",
    "    return {\n",
    "        movie_id: tokenize(content.storyline, stopwords)\n",
    "        for movie_id, content in contents.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vocabulary(corpus):\n",
    "    \"\"\"Construct vocabulary by given corpus\n",
    "    \n",
    "    For each document in corpus, for each word in the document\n",
    "    a (document id, term frequency) tuple added to vocabulary dict.\n",
    "    So, example resulted dictionary would be:\n",
    "    \n",
    "    {\n",
    "        \"example_word\": [\n",
    "            N / document frequency of example_word\n",
    "            (\"imdb-id-1\", 12),\n",
    "            (\"imdb-id-2\", 2),\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    }\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: dict\n",
    "        Corpus which contains documents\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vocab: dict\n",
    "        Vocabulary dictionary\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    N = len(corpus)\n",
    "    for doc_id, doc in corpus.items():\n",
    "        for word, count in dict(Counter(doc)).items():\n",
    "            vocab.setdefault(word, []).append((doc_id, count))\n",
    "    return {\n",
    "        word: [N / len(postings)] + postings\n",
    "        for word, postings in vocab.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tf_idf_score(vocabulary):\n",
    "    \"\"\"Get total number of occurences for each word in vocabulary\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary: dict\n",
    "        Vocabulary of words\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of word: max tf-idf score of word\n",
    "    \"\"\"\n",
    "    tf_idfs = {}\n",
    "    for word, postings in vocabulary.items():\n",
    "        idf = log(postings[0])\n",
    "        for doc, tf in postings[1:]:\n",
    "            tf_idfs.setdefault(word, []).append(tf * idf)\n",
    "\n",
    "    return {\n",
    "        word: max(tf_idf_scores)\n",
    "        for word, tf_idf_scores in tf_idfs.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_words(k, vocabulary):\n",
    "    \"\"\"Returns top tf-idf scored k words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of words to select\n",
    "    vocabulary: dict\n",
    "        Vocabulary of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of most tf-idf scored `k` words\n",
    "    \"\"\"\n",
    "    max_tf_idf_scores = get_max_tf_idf_score(vocabulary)\n",
    "\n",
    "    return list(nlargest(k, max_tf_idf_scores, key=max_tf_idf_scores.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(vocabulary, threshold):\n",
    "    \"\"\"Returns top tf-idf scoerd words\n",
    "    \n",
    "    It calculates max tf-idf score of each word, then returns\n",
    "    words that their tf-idf scores are greater than some `threshold`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary: dict\n",
    "        Vocabulary of words\n",
    "    threshold: float\n",
    "        Threshold for tf-idf scores\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of words that they are above of some `threshold`\n",
    "    \"\"\"\n",
    "    max_tf_idf_scores = get_max_tf_idf_score(vocabulary)\n",
    "    \n",
    "    return [word for word, max_tf_idf in max_tf_idf_scores.items() if max_tf_idf > threshold]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_document_lengths(vocabulary, feature_words):\n",
    "    \"\"\"Construct document length dict\n",
    "    \n",
    "    Calculate length of each document, which is normalization factor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary: dict\n",
    "        Postings dict of all words\n",
    "    feature_words\n",
    "        List of words to encode documents\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    dict\n",
    "        Length of each document\n",
    "    \"\"\"\n",
    "    \n",
    "    lengths = {}\n",
    "\n",
    "    for word in feature_words:\n",
    "        postings = vocabulary[word]\n",
    "        idf = log(postings[0])\n",
    "        for doc, tf in postings[1:]:\n",
    "            if doc in lengths:\n",
    "                lengths[doc] += (tf * idf) ** 2\n",
    "            else:\n",
    "                lengths[doc] = (tf * idf) ** 2\n",
    "    \n",
    "    return {\n",
    "        doc: norm_squared ** 0.5\n",
    "        for doc, norm_squared in lengths.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = construct_corpus(all_movie_contents)\n",
    "\n",
    "vocabulary = construct_vocabulary(corpus)\n",
    "\n",
    "max_tf_idf_scores = get_max_tf_idf_score(vocabulary)\n",
    "\n",
    "feature_words = top_words(vocabulary, TF_IDF_THRESHOLD)\n",
    "# To use top `NUMBER_WORDS` as encoding words comment out above line\n",
    "# Uncomment below line\n",
    "# feature_words = top_k_words(NUMBER_WORDS ,vocabulary)\n",
    "\n",
    "lengths = construct_document_lengths(vocabulary, feature_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "In the recommendation part,\n",
    "\n",
    "-  First, program gets storyline from IMDB site for the movie\n",
    "-  Then, it computes normalized vector representation for this storyline\n",
    "-  Calculate similarity to each element in the corpus\n",
    "-  Recommend a most similar K movies from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vector(text):\n",
    "    \"\"\"Constructs vector for given text\n",
    "    \n",
    "    It calculates tf-idf scores and normalize vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        Text to compute vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Sparse normalized vector of text\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    vector = {}\n",
    "    for word in feature_words:\n",
    "        if word in tokens:\n",
    "            tf = tokens.count(word)\n",
    "            idf = log(vocabulary[word][0])\n",
    "            vector[word] = tf * idf\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_k_similar(k, text_vector):\n",
    "    \"\"\"Gets most similar movies\n",
    "    \n",
    "    Calculate similarities between `text_vector` and each document in the corpus\n",
    "    and returns most similar `k` elements\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        Number of recommendations\n",
    "    text_vector: dict\n",
    "        Sparse vector representation of a text\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of most similar documents' ids\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for word, score in text_vector.items():\n",
    "        idf = log(vocabulary[word][0])\n",
    "        for doc, tf in vocabulary[word][1:]:\n",
    "            if doc in scores:\n",
    "                scores[doc] += tf * idf * score\n",
    "            else:\n",
    "                scores[doc] = tf * idf * score\n",
    "    \n",
    "    scores = {doc_id: score / lengths[doc_id] for doc_id, score in scores.items()}\n",
    "\n",
    "    return list(nlargest(k, scores, key=scores.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(imdb_id):\n",
    "    \"\"\"\n",
    "    Gets an imdb id and returns a list of recommended movie ids for that movie. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imdb_id: str\n",
    "        IMDB id\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of recommended movie ids for that movie\n",
    "    \"\"\"\n",
    "    _, storyline, _ = get_movie_contents(imdb_id)\n",
    "    \n",
    "    text_vector = construct_vector(storyline)\n",
    "    \n",
    "    recommendations = get_most_k_similar(NUMBER_RECS + 1, text_vector)\n",
    "    if imdb_id in recommendations:\n",
    "        recommendations.remove(imdb_id)\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In the evaluation part, for given recomended movie ids and relevant movie ids(which are IMDB recommendations), I have calculated 3 metrics <b>Precision</b>, <b>Recall</b>, and <b>F-1 score</b>.\n",
    "\n",
    "In last method, <b>evaluation_system</b>, for a given movie id, I have tested system for different K values and printed out each metric's result for each K\n",
    "\n",
    "K: $1, 2, 3, 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(rec_movie_ids, relevant_movie_ids, K):\n",
    "    \"\"\"Compute Precision\"\"\"\n",
    "    count = 0\n",
    "    for movie_id in relevant_movie_ids:\n",
    "        if movie_id in rec_movie_ids[:K]:\n",
    "            count += 1\n",
    "\n",
    "    return count / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(rec_movie_ids, relevant_movie_ids, K):\n",
    "    \"\"\"Compute Recall\"\"\"\n",
    "    count = 0\n",
    "    for movie_id in rec_movie_ids[:K]:\n",
    "        if movie_id in relevant_movie_ids:\n",
    "            count += 1\n",
    "            \n",
    "    return count / (len(relevant_movie_ids) + 10**-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(rec_movie_ids, relevant_movie_ids, K):\n",
    "    \"\"\"\n",
    "    Gets list of recommended and relevant movie ids and K value.\n",
    "    \n",
    "    Returns precision, recall, F1 values for K respectively. \n",
    "    \"\"\"\n",
    "    prec = precision(rec_movie_ids, relevant_movie_ids, K) \n",
    "    rec = recall(rec_movie_ids, relevant_movie_ids, K)\n",
    "    F_1 = 2 * prec * rec / (prec + rec + 10**(-8))\n",
    "    \n",
    "    return prec, rec, F_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(movie_id):\n",
    "    \"\"\"Evaluate system for given movie id\"\"\"\n",
    "    rec_movie_ids = recommend(movie_id)\n",
    "    relevant_movie_ids = all_movie_contents[movie_id].recommendations\n",
    "\n",
    "    for K in (1, 2, 3, 10):\n",
    "        prec, rec, f_1 = evaluate_recommendations(rec_movie_ids, relevant_movie_ids, K)\n",
    "        print(\"Precision: {}, Recall: {}, F-1: {}\".format(prec, rec, f_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = read_csv(MOVIE_ID_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0, Recall: 0.0, F-1: 0.0\n",
      "Precision: 0.5, Recall: 0.08333333326388888, F-1: 0.14285714030612245\n",
      "Precision: 0.3333333333333333, Recall: 0.08333333326388888, F-1: 0.13333333004444453\n",
      "Precision: 0.4, Recall: 0.3333333330555555, F-1: 0.3636363585123967\n"
     ]
    }
   ],
   "source": [
    "evaluate_system(movie_ids[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
