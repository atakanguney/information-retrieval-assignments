{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:39:50.714773Z",
     "start_time": "2019-04-27T15:39:50.706690Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "try:\n",
    "    from rouge import Rouge\n",
    "except:\n",
    "    !pip install rouge\n",
    "    from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:50:41.684513Z",
     "start_time": "2019-04-27T15:50:41.669923Z"
    }
   },
   "outputs": [],
   "source": [
    "VECTOR_PATH = '../glove.6B.200d.pkl'\n",
    "ARTICLES_PATH = '../articles/'\n",
    "GOLD_SUMMARIES_PATH = '../gold_summaries/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:58:14.216495Z",
     "start_time": "2019-04-27T15:58:14.090509Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_documents(doc_folder):\n",
    "    \"\"\"Read documents\n",
    "    \n",
    "    Read documents in the given `doc_folder`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_folder: str\n",
    "        Document folder path\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Documents\n",
    "    \"\"\"\n",
    "    article_names = os.listdir(doc_folder)\n",
    "    article_names.sort()\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    for article_name in article_names:\n",
    "        path = os.path.join(doc_folder, article_name)\n",
    "        with open(path, \"rb\") as f:\n",
    "            try:\n",
    "                doc = f.read().decode(\"utf-8\")\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(article_name, \" has unknown encoding\")\n",
    "                raise e\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T14:56:59.045145Z",
     "start_time": "2019-04-27T14:56:59.014321Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_document_to_sentence(doc):\n",
    "    \"\"\"Parse document to sentences\n",
    "    \n",
    "    This parser assumes every sentence begins with upper-case or digit and\n",
    "    ends with ., ..., ?, ! or whitespace and after each sentence there \n",
    "    must be a whitespace.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: str\n",
    "        Document string to parse\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Sentences\n",
    "    \"\"\"\n",
    "    pat = re.compile(r\"([A-Z0-9].*?)[\\.?!\\s]+\\s\", re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    return pat.findall(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:20:28.736296Z",
     "start_time": "2019-04-27T15:20:28.711558Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_documents_to_sentences(docs_path):\n",
    "    \"\"\"Parse documents\n",
    "\n",
    "    Extract sentences for given documents folders path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs_path: str\n",
    "        Path of folder that contains documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Documents parsed to their sentences as numpy array\n",
    "    \"\"\"\n",
    "    docs = read_documents(docs_path)\n",
    "    return [np.array(parse_document_to_sentence(doc)) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:21:40.866371Z",
     "start_time": "2019-04-27T15:21:40.855502Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent2vec(sent, word2vec):\n",
    "    \"\"\"Sentence to vector\n",
    "    \n",
    "    Transform plain sentence to its vector representation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sent: str\n",
    "        Sentence\n",
    "    word2vec: dict\n",
    "        Word vectors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Vector representation of sentence\n",
    "    \"\"\"\n",
    "    tokens = sent.lower().split()\n",
    "    \n",
    "    return np.mean(\n",
    "        [\n",
    "            word2vec[token] if token in word2vec\n",
    "            else word2vec[\"unk\"]\n",
    "            for token in tokens\n",
    "        ],\n",
    "        axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:21:41.334751Z",
     "start_time": "2019-04-27T15:21:41.328866Z"
    }
   },
   "outputs": [],
   "source": [
    "def doc2vec(docs_by_sentences, word2vec):\n",
    "    \"\"\"Documents to vectors\n",
    "\n",
    "    Convert sentences of documents to vectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs_by_sentences: list\n",
    "        List of list of sentences\n",
    "\n",
    "    word2vec: dict\n",
    "        Word vectors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of numpy.array, each numpy.array is document representation by its sentences\n",
    "    \"\"\"\n",
    "    docs_as_vectors = []\n",
    "\n",
    "    return [np.array([sent2vec(sent, word2vec) for sent in doc]) for doc in docs_by_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T10:28:57.979274Z",
     "start_time": "2019-04-27T10:27:56.143745Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(VECTOR_PATH, \"rb\") as f:\n",
    "    word_vectors = pickle.load(f)\n",
    "\n",
    "docs = read_documents(ARTICLES_PATH)\n",
    "\n",
    "sentences = parse_documents_to_sentences(ARTICLES_PATH)\n",
    "\n",
    "doc_vectors = doc2vec(sentences, word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "## Implementation\n",
    "\n",
    "The loss function is:\n",
    "\n",
    "$\\mathcal{L}$ = ${\\left|\\left| X - CM \\right|\\right|}_{F}$\n",
    "\n",
    "$N$ is the number of sample, $D$ is the input dimension, $k$ is the number of clusters\n",
    "\n",
    "$C \\in \\{0, 1\\}^{N \\times k}$, $M \\in \\mathbb{R}^{k \\times D}$\n",
    "\n",
    "The only constraint we have is $\\sum_{i}{C_{n, i} = 1}$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}^2 &= {\\left|\\left| X - CM \\right|\\right|}_{F}^2 \\\\\n",
    "                &= trace((X - CM)^T (X - CM)) \\\\\n",
    "                &= trace(X^T X - X^T CM - M^T C^T X + M^T C^T C M)\n",
    "\\end{align*}\n",
    "\n",
    "The partial derivative w.r.t $M$ follows\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}^2}{\\partial M} &= \\frac{\\partial trace(X^T X - X^T CM - M^T C^T X + M^T C^T C M)}{\\partial M} \\\\\n",
    "                                            &= \\frac{\\partial trace(X^T X)}{\\partial M} + \\frac{\\partial trace(- X^T CM)}{\\partial M} + \\frac{\\partial trace(- M^T C^T X)}{\\partial M} + \\frac{\\partial trace(M^T C^T C M)}{\\partial M} \\\\\n",
    "                                            &= 0 - C^T X - C^T X + 2 C^T C M \\\\\n",
    "                                            &= -2 C^T X + 2 C^T C M\n",
    "\\end{align*}\n",
    "\n",
    "And let derivative equals 0\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}^2}{\\partial M} &= -2 C^T X + 2 C^T C M = 0 \\\\\n",
    "    &\\Rightarrow C^T C M = C^T X \\\\\n",
    "    &\\Rightarrow M = (C^T C)^{-1} C^T X\n",
    "\\end{align*}\n",
    "\n",
    "Clusters have been assigned w.r.t. distance by calculated cluster means, which are rows of matrix $M$. So, for each example number $n$, corresponding value in assignment matrix is calculated as $C_{n, k} = 1$ and $C_{n, i} = 0$ for $k \\neq i$, where $k = \\arg\\min_{k}{\\left|\\left| X_{n, :} - M_{k, :} \\right|\\right|}$. So, we can find cluster assignment in vectorized way as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\arg\\min_{k}{{\\left|\\left| X_{n} - M_{k} \\right|\\right|}_2} &=  \\arg\\min_{k}{{\\left|\\left| X_{n} - M_{k} \\right|\\right|}_2^2}\\\\\n",
    "                                                   &= \\arg\\min_{k}{\\sum_{i}{(X_{n, i} - M_{k, i})^2}} \\\\\n",
    "                                                   &= \\arg\\min_{k}{\\sum_{i}{X_{n, i}^2 - 2 X_{n, i} M_{k, i} + M_{k, i}^2}} \\\\\n",
    "                                                   &= \\arg\\min_{k}{\\sum_{i}{- 2 X_{n, i} M_{k, i} + M_{k, i}^2}} \\\\\n",
    "                                                   &= \\arg\\min_{k}{\\sum_{i}{- 2 X_{n, i} M^T_{i, k} + M_{k, i}^2}} \\\\\n",
    "                                                   &= \\arg\\min_{k}{- 2 {X M^T}_{n, k} + \\sum_{i}{M_{k, i}^2}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T16:15:12.459661Z",
     "start_time": "2019-04-27T16:15:12.410985Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_onehot(vec, k):\n",
    "    \"\"\"Convert to onehot\n",
    "    \n",
    "    Converts `vec` to its onehot representation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec: numpy.ndarray\n",
    "    \n",
    "    k: int\n",
    "        Size of onehot vectors\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Onehot representation of given array\n",
    "    \"\"\"\n",
    "    one_hot_mtx = np.zeros((vec.shape[0], k))\n",
    "    for sample, idx in enumerate(vec):\n",
    "        one_hot_mtx[sample, idx] = 1\n",
    "        \n",
    "    return one_hot_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:32:28.378750Z",
     "start_time": "2019-04-27T15:32:28.366207Z"
    }
   },
   "outputs": [],
   "source": [
    "def K_means(data, k, verbose=False):\n",
    "    \"\"\"K-Means clustering\n",
    "    \n",
    "    Given the `data` in shape (N, D) where N is the number of examples\n",
    "    and D is the number of features, it calculates `k` cluster center\n",
    "    that represents the data most. Maths behind this implementation is detailed\n",
    "    above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray\n",
    "        Data\n",
    "    k: int\n",
    "        Number of Clusters\n",
    "    Verbose: bool\n",
    "        Print loss in each iteration if True\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M: numpy.ndarray\n",
    "        Cluster centers\n",
    "    C: numpy.ndarray\n",
    "        Data assignments\n",
    "    loss: float\n",
    "        Last calculated loss\n",
    "    \"\"\"\n",
    "    num_samples, input_dim = data.shape\n",
    "    \n",
    "\n",
    "    M = np.random.rand(k, input_dim)\n",
    "    C = to_onehot(np.argmin(-2 * np.dot(data, M.T) + np.power(M, 2).sum(axis=1)[np.newaxis, :], axis=1), k)\n",
    "    prev_C = np.zeros_like(C)\n",
    "    \n",
    "    loss = np.linalg.norm(data - np.dot(C, M), ord=\"fro\")\n",
    "    \n",
    "    while not np.all(C == prev_C):\n",
    "        if verbose:\n",
    "            print(\"Loss: {}\".format(loss))\n",
    "        prev_C = C\n",
    "        \n",
    "        M = np.dot(np.dot(np.linalg.pinv(np.dot(C.T, C)), C.T), data)\n",
    "        C = to_onehot(np.argmin(-2 * np.dot(data, M.T) + np.power(M, 2).sum(axis=1)[np.newaxis, :], axis=1), k)\n",
    "\n",
    "        loss = np.linalg.norm(data - np.dot(C, M), ord=\"fro\")\n",
    "        \n",
    "    return M, C, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing K-Means\n",
    "\n",
    "In this part, I have created fake dataset and tested my implementation of k-means clustering algorithm with the fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T16:19:51.893954Z",
     "start_time": "2019-04-27T16:19:51.046951Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T16:19:52.406548Z",
     "start_time": "2019-04-27T16:19:52.163031Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 Different centers\n",
    "data1 = np.random.rand(50, 2) * 4 + 5\n",
    "data2 = np.random.rand(30, 2) * 4\n",
    "data3 = np.random.rand(40, 2) * 4 + 10\n",
    "\n",
    "data = np.concatenate([data1, data2, data3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:32:29.926057Z",
     "start_time": "2019-04-27T15:32:29.548896Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(data[:, 0], data[:, 1], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot dataset with cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:32:33.225539Z",
     "start_time": "2019-04-27T15:32:32.901804Z"
    }
   },
   "outputs": [],
   "source": [
    "M, C, loss = K_means(data, 3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(data[:, 0], data[:, 1], \"o\")\n",
    "plt.plot(M[:, 0], M[:, 1], \"rx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:32:41.881242Z",
     "start_time": "2019-04-27T15:32:41.876138Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_most_repr_sent_in_cluster(data, M):\n",
    "    \"\"\"Find Most Representative Sentences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray\n",
    "        Document representation\n",
    "        \n",
    "    M: numpy.ndarray\n",
    "        Cluster centers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Indices of sentences which are most representative ones\n",
    "    \"\"\"\n",
    "    return np.argmin(-2 * np.dot(data, M.T) + np.power(M, 2).sum(axis=1)[np.newaxis, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:32:42.468249Z",
     "start_time": "2019-04-27T15:32:42.461154Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster(doc_vectors, num_k_means_trial=5):\n",
    "    \"\"\"Cluster and return most represntative sentence indices\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_vectors: numpy.ndarray\n",
    "        Document representation by sentence vectors\n",
    "        \n",
    "    num_k_means_trial: int\n",
    "        Number of k-means trial select best one\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        List of most representative indices\n",
    "    \"\"\"\n",
    "    k = max(1, int(len(doc_vectors) ** 0.5))\n",
    "\n",
    "    k_means = []\n",
    "    min_loss_model = -1\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "    for i in range(num_k_means_trial):\n",
    "        M, C, loss = K_means(doc_vectors, k)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            min_loss_model = i\n",
    "            \n",
    "        k_means.append(M)\n",
    "        \n",
    "    best_means = k_means[min_loss_model]\n",
    "    repr_sentences = find_most_repr_sent_in_cluster(doc_vectors, best_means)\n",
    "\n",
    "    return repr_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:33:13.759572Z",
     "start_time": "2019-04-27T15:33:13.755867Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_summary(doc_vectors, doc_sentences):\n",
    "    \"\"\"Extracts Summary\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_vectors: numpy.ndarray\n",
    "        Document representation by sentence vectors\n",
    "    doc_sentences: numpy.ndarray\n",
    "        Document representation by its sentences\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Summary of document\n",
    "    \"\"\"\n",
    "    most_repr_sentences = cluster(doc_vectors)\n",
    "    return \". \".join(list(set(doc_sentences[most_repr_sentences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:43:40.180652Z",
     "start_time": "2019-04-27T15:43:25.621963Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries = [extract_summary(doc_vector, doc_sentences) for doc_vector, doc_sentences in zip(doc_vectors, sentences)]\n",
    "gold_summaries = read_documents(GOLD_SUMMARIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:48:07.726846Z",
     "start_time": "2019-04-27T15:48:07.720391Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(gold_summaries, extracted_summaries):\n",
    "    \"\"\"Evaluation of summaries\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gold_summaries: list\n",
    "        Reference summaries\n",
    "    extracted_summaries: list\n",
    "        Extracted summaries by the model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Rough scores as tuple (rouge-1.f, rouge-2.f, rouge-l.f)\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    for hyp, ref in zip(gold_summaries, extracted_summaries):\n",
    "        scores = rouge.get_scores(hyp, ref)[0]\n",
    "        rouge_scores.append((scores[\"rouge-1\"][\"f\"], scores[\"rouge-2\"][\"f\"], scores[\"rouge-l\"][\"f\"]))\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:50:18.537470Z",
     "start_time": "2019-04-27T15:50:18.516468Z"
    }
   },
   "outputs": [],
   "source": [
    "r_scores = evaluation(gold_summaries, summaries)\n",
    "r_scores_avg = np.array(r_scores).mean(axis=0)\n",
    "r_scores_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct at least two different models and compare their results. End with a conclusion. You are\n",
    "# encouraged to use plots/histograms etc. for coparison or evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
